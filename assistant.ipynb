{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d291acc1fb4c7db72419fcddbe273f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, StoppingCriteria, StoppingCriteriaList, TextStreamer\n",
    "from datetime import datetime\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnMessageEnd(StoppingCriteria):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.end_tokens = {tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"\\n\\n\")}\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        if input_ids.shape[1] < 2:\n",
    "            return False\n",
    "        last_token = input_ids[0, -1].item()\n",
    "        penultimate_token = input_ids[0, -2].item()\n",
    "        # Ensure proper message-end stopping\n",
    "        return (\n",
    "            last_token in self.end_tokens or \n",
    "            (last_token == self.tokenizer.convert_tokens_to_ids(\"\\n\") and penultimate_token == self.tokenizer.convert_tokens_to_ids(\"\\n\"))\n",
    "        )\n",
    "\n",
    "\n",
    "# Define the stopping criteria\n",
    "stopping_criteria = StoppingCriteriaList([StopOnMessageEnd(tokenizer)])\n",
    "streamer = TextStreamer(tokenizer, model, stopping_criteria=stopping_criteria, max_length=256, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting current location...\n",
      "Location: Winterthur, Zurich, CH\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "def get_current_location():\n",
    "    try:\n",
    "        response = requests.get('https://ipinfo.io')\n",
    "        data = response.json()\n",
    "        return data['city'] + \", \" + data['region'] + \", \" + data['country']\n",
    "    except Exception as e:\n",
    "        return \"Unknown Location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 19\u001b[0m\n\u001b[0;32m      1\u001b[0m conversation_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m<|start_header_id|>system<|end_header_id|>\u001b[39m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m<|eot_id|>\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Get user input\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Exit condition\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_text\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\vaiu\\.conda\\envs\\pytorch-env\\lib\\site-packages\\ipykernel\\kernelbase.py:1270\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1268\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vaiu\\.conda\\envs\\pytorch-env\\lib\\site-packages\\ipykernel\\kernelbase.py:1313\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "conversation_history = f\"\"\"\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023.\n",
    "\n",
    "You are a highly competent and precise virtual AI assistant named James. \n",
    "Your goal is to provide fact-based, clear, and concise answers to the user's questions.\n",
    "Your replies are correct and don't show inconsistencies or contradictions.\n",
    "\n",
    "Context Information:\n",
    "The current day is {datetime.now().strftime(\"%B %d, %Y\")}.\n",
    "The current location is {get_current_location()}.\n",
    "\n",
    "<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    input_text = input(\"User: \")\n",
    "\n",
    "    # Exit condition\n",
    "    if input_text.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Append user input to conversation history\n",
    "    conversation_history += f\"<|start_header_id|>user<|end_header_id|>\\n{input_text} <|eot_id|>\\n\"\n",
    "\n",
    "    # Prepare prompt for the model\n",
    "    prompt = conversation_history + \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the response\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=300,\n",
    "        num_return_sequences=1,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.pad_token_id,  # Explicitly set pad_token_id\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=2,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Append assistant response to conversation history\n",
    "    conversation_history += f\"{response} <|eot_id|>\\n\"\n",
    "\n",
    "    # Print the assistant's response\n",
    "    print(f\"James: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "What's your name?\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023.\n",
    "\n",
    "You are a highly competent and precise virtual AI assistant named James. \n",
    "Your goal is to provide fact-based, clear, and concise answers to the user's questions.\n",
    "Your replies are correct and don't show inconsistencies or contradictions.\n",
    "\n",
    "Context Information:\n",
    "The current day is {datetime.now().strftime(\"%B %d, %Y\")}.\n",
    "The current location is {get_current_location()}.\n",
    "\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Hello, who are you? <|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Hello! My name is James, I'm your personal AI assistant. How can I help you today? <|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Can you help me with some questions? <|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Of course, I'd be happy to help. What do you need assistance with? <|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{input_text} <|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate the response with attention mask and stopping criteria\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=300,  # Adjust based on your requirement\n",
    "    num_return_sequences=1,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    no_repeat_ngram_size=2,\n",
    "    streamer=streamer,\n",
    ")\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
